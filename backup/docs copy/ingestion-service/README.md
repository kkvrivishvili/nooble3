# Ingestion Service

## Descripci√≥n

Ingestion Service es un microservicio especializado responsable de todo el proceso de ingesta de documentos para el sistema RAG (Retrieval Augmented Generation). El servicio maneja la recepci√≥n, procesamiento, extracci√≥n de texto, chunking y orquestaci√≥n del almacenamiento de documentos y sus embeddings correspondientes.

## üèóÔ∏è Ecosistema de Servicios

La arquitectura se organiza en 3 niveles jer√°rquicos:

### Nivel 1: Orquestaci√≥n

- **Agent Orchestrator**: Punto de entrada √∫nico, gesti√≥n de sesiones y coordinaci√≥n global

### Nivel 2: Servicios Funcionales

- **Conversation Service**: Historial y contexto de conversaciones
- **Workflow Engine**: Flujos de trabajo complejos multi-etapa
- **Agent Execution**: L√≥gica espec√≠fica del agente
- **Tool Registry**: Registro y ejecuci√≥n de herramientas

### Nivel 3: Servicios de Infraestructura

- **Query Service**: Procesamiento RAG y LLM
- **Embedding Service**: Generaci√≥n de embeddings vectoriales
- **Ingestion Service**: Procesamiento de documentos

> üìå **Este documento describe el Ingestion Service**, ubicado en el Nivel 3 como servicio de infraestructura especializado en el procesamiento y preparaci√≥n de documentos para el sistema RAG

## Caracter√≠sticas

- Soporte para m√∫ltiples formatos de documentos (PDF, Word, Excel, texto, HTML, im√°genes, etc.)
- Procesamiento as√≠ncrono mediante sistema de colas
- Chunking inteligente optimizado para RAG
- Extracci√≥n de texto con reconocimiento de estructura
- Gesti√≥n de colecciones de documentos
- Ingesta desde m√∫ltiples fuentes (archivos, URLs, texto plano)
- Soporte para procesamiento por lotes
- Tracking centralizado de tokens y uso de recursos

## üîÑ Flujos de Trabajo Principales

### 1. Ingesti√≥n de Documentos (Flujo principal)
```
Cliente ‚Üí Orchestrator ‚Üí Workflow Engine ‚Üí Ingestion Service ‚Üí Embedding Service ‚Üí Notificaci√≥n de completado
```

### 2. Actualizaci√≥n de Colecciones
```
Cliente ‚Üí Orchestrator ‚Üí Ingestion Service ‚Üí Actualizaci√≥n de metadatos ‚Üí Notificaci√≥n
```

> üîç **Rol del Ingestion Service**: Procesar documentos en diversos formatos, extraer texto estructurado, dividirlo en chunks optimizados para RAG y coordinar la generaci√≥n de embeddings y almacenamiento.

## Arquitectura

El servicio sigue una arquitectura de procesamiento modular:

```
Cliente ‚Üí Ingestion Service ‚Üí Cola de procesamiento
                ‚Üì
           Validaci√≥n
                ‚Üì
      Extracci√≥n de texto
                ‚Üì
      Chunking inteligente
                ‚Üì
      Embedding Service ‚Üê Solicitud de embeddings
                ‚Üì
      Almacenamiento en BD vectorial
                ‚Üì
      Notificaci√≥n de completado
```

### Componentes Principales

- **routes/ingestion.py**: Endpoints principales para carga de documentos
- **routes/documents.py**: Gesti√≥n de documentos existentes
- **routes/collections.py**: Gesti√≥n de colecciones de documentos
- **routes/jobs.py**: Monitoreo de trabajos de procesamiento
- **services/chunking.py**: Procesamiento y divisi√≥n de documentos
- **services/embedding.py**: Cliente del servicio de embeddings
- **services/queue.py**: Sistema de colas para procesamiento as√≠ncrono
- **services/storage.py**: Gesti√≥n de almacenamiento de archivos
- **services/worker.py**: Workers para procesamiento en segundo plano

## üö¶ Sistema de Colas Multi-tenant

### Estructura Jer√°rquica de Colas del Ingestion Service

```
+--------------------------------------------------+
|             COLAS DE INGESTION                   |
+--------------------------------------------------+
|                                                  |
| ingestion.tasks.{tenant_id}                      | ‚Üí Cola principal de tareas
| ingestion.batch.{tenant_id}.{batch_id}           | ‚Üí Lotes de documentos
| ingestion.status.{tenant_id}.{job_id}            | ‚Üí Estado de procesamiento
| ingestion.collection.{tenant_id}.{collection_id} | ‚Üí Metadatos de colecci√≥n 
|                                                  |
+--------------------------------------------------+
```

> **Nota**: Los nombres de colas siguen la convenci√≥n est√°ndar `{service}.{tipo}.{tenant_id}[.{id_adicional}]` para mantener consistencia a trav√©s de todo el ecosistema de microservicios.

### Caracter√≠sticas Clave

- **Segmentaci√≥n por tenant**: Completo aislamiento de datos entre tenants
- **Procesamiento por lotes**: Optimizaci√≥n para grandes vol√∫menes de documentos
- **Tracking detallado**: Seguimiento granular del estado de cada documento
- **Reintentos autom√°ticos**: Recuperaci√≥n ante fallos en procesamiento

### Formato de Mensaje Estandarizado

```json
{
  "job_id": "uuid-v4",
  "tenant_id": "tenant-identifier",
  "collection_id": "collection-identifier",
  "created_at": "ISO-timestamp",
  "status": "pending|processing|completed|failed",
  "type": "document_ingestion|metadata_update|collection_management",
  "priority": 0-9,
  "metadata": {
    "source": "upload|url|text|batch",
    "document_type": "pdf|docx|txt|html|...",
    "workflow_id": "optional-workflow-id"
  },
  "payload": {
    "file_path": "path/to/document", 
    "document_id": "optional-document-id",
    "chunking_strategy": "recursive|sentence|paragraph|...",
    "chunk_size": 1000,
    "chunk_overlap": 200
  }
}
```

## Instalaci√≥n

```bash
# Clonar el repositorio
git clone <repositorio>

# Instalar dependencias
cd ingestion-service
pip install -r requirements.txt

# Ejecutar el servicio
python main.py
```

## Configuraci√≥n

El servicio utiliza variables de entorno para su configuraci√≥n:

```bash
# Configuraci√≥n del servicio
SERVICE_NAME=ingestion-service
SERVICE_VERSION=1.0.0
LOG_LEVEL=INFO

# Conexiones externas
SUPABASE_URL=
SUPABASE_KEY=
REDIS_URL=redis://redis:6379/0

# Configuraci√≥n de procesamiento
MAX_WORKERS=4
MAX_QUEUE_SIZE=1000
MAX_DOC_SIZE_MB=20
DEFAULT_CHUNK_SIZE=1000
DEFAULT_CHUNK_OVERLAP=200

# URL del servicio de embeddings
EMBEDDING_SERVICE_URL=http://embedding-service:8000
```

## API

### Endpoints Principales

#### Ingesta de Documentos

| Endpoint | M√©todo | Descripci√≥n |
|----------|--------|-------------|
| `/upload` | POST | Carga y procesa un archivo de documento |
| `/ingest/url` | POST | Procesa y extrae contenido de una URL |
| `/ingest/text` | POST | Procesa texto plano como documento |
| `/ingest/batch/urls` | POST | Procesa un lote de URLs en segundo plano |

#### Gesti√≥n de Documentos

| Endpoint | M√©todo | Descripci√≥n |
|----------|--------|-------------|
| `/documents` | GET | Lista documentos por colecci√≥n |
| `/documents/{document_id}` | GET | Obtiene metadatos de un documento |
| `/documents/{document_id}` | DELETE | Elimina un documento |
| `/documents/{document_id}/content` | GET | Obtiene el contenido de un documento |

#### Gesti√≥n de Colecciones

| Endpoint | M√©todo | Descripci√≥n |
|----------|--------|-------------|
| `/collections` | GET | Lista colecciones disponibles |
| `/collections` | POST | Crea una nueva colecci√≥n |
| `/collections/{collection_id}` | GET | Obtiene detalles de una colecci√≥n |
| `/collections/{collection_id}` | DELETE | Elimina una colecci√≥n y sus documentos |

#### Monitoreo de Trabajos

| Endpoint | M√©todo | Descripci√≥n |
|----------|--------|-------------|
| `/jobs/{job_id}` | GET | Obtiene estado de un trabajo de procesamiento |
| `/jobs` | GET | Lista trabajos recientes por tenant |

### Ejemplo de Uso

#### Carga de Documento

```python
import requests

# Datos del documento
files = {"file": open("documento.pdf", "rb")}
data = {
    "collection_id": "mi-coleccion",
    "title": "Mi Documento",
    "description": "Descripci√≥n del documento",
    "tags": "tag1,tag2"
}

# Enviar solicitud
response = requests.post(
    "http://ingestion-service:8000/upload",
    files=files,
    data=data,
    headers={"x-tenant-id": "tenant123"}
)

# Verificar resultado
if response.status_code == 202:
    job_id = response.json().get("job_id")
    print(f"Documento en procesamiento. Job ID: {job_id}")
```

## üîä Sistema de Notificaciones

### WebSockets Centralizados

- **Integraci√≥n con orquestador**: Conexi√≥n bidireccional con Agent Orchestrator
- **Eventos de progreso**: Actualizaciones en tiempo real del estado de ingesti√≥n
- **Reconexi√≥n autom√°tica**: Mecanismo de backoff exponencial para mayor resiliencia
- **Autenticaci√≥n por token**: Comunicaci√≥n segura entre servicios

### Eventos WebSocket del Ingestion Service

#### Eventos Estandarizados (Para comunicaci√≥n con el Orchestrator)

- `task_status_update`: Actualiza el estado de procesamiento (por ejemplo: "procesando chunk 5 de 10")
- `task_completed`: Ingesti√≥n de documento(s) completada exitosamente
- `task_failed`: Error en el proceso de ingesti√≥n

#### Eventos Espec√≠ficos (Para procesamiento interno)

- `document_chunking_completed`: Documento dividido en chunks para procesamiento
- `collection_updated`: Se ha actualizado una colecci√≥n con nuevos documentos
- `metadata_extraction_completed`: Se han extra√≠do metadatos de documentos

> **Importante**: Los eventos estandarizados siguen el formato com√∫n definido por el Agent Orchestrator Service para mantener consistencia en todo el ecosistema de microservicios.

### Implementaci√≥n WebSocket para Notificaciones:

```python
# websocket/notifier.py
import asyncio
import websockets
import json
import logging
import os
from datetime import datetime

logger = logging.getLogger(__name__)

class IngestionNotifier:
    def __init__(self):
        self.service_name = "ingestion-service"
        self.orchestrator_url = "ws://agent-orchestrator:8000/ws/task_updates"
        self.service_token = os.getenv("SERVICE_TOKEN")
        self.reconnect_delay = 1.0  # segundos, con backoff
        self.websocket = None
        self.connected = False
        
    async def connect(self):
        """Establece conexi√≥n con orquestrador con reconexi√≥n autom√°tica"""
        while True:
            try:
                logger.info(f"Conectando a {self.orchestrator_url}")
                async with websockets.connect(self.orchestrator_url) as ws:
                    # Autenticarse como servicio
                    await ws.send(json.dumps({
                        "service_token": self.service_token,
                        "service_name": self.service_name
                    }))
                    
                    # Esperar confirmaci√≥n
                    auth_response = await ws.recv()
                    if json.loads(auth_response).get("status") != "authenticated":
                        logger.error("Fallo en la autenticaci√≥n WebSocket")
                        raise Exception("Authentication failed")
                    
                    logger.info(f"Conexi√≥n WebSocket establecida para {self.service_name}")
                    # Conexi√≥n establecida
                    self.reconnect_delay = 1.0  # reset backoff
                    self.websocket = ws
                    self.connected = True
                    
                    # Mantener conexi√≥n abierta
                    while True:
                        # Keep-alive o esperar cierre
                        await asyncio.sleep(30)
                        await ws.ping()
                        
            except Exception as e:
                self.connected = False
                logger.warning(f"Error en conexi√≥n WebSocket: {e}. Reintentando en {self.reconnect_delay}s")
                # Implementar backoff exponencial
                await asyncio.sleep(self.reconnect_delay)
                self.reconnect_delay = min(30.0, self.reconnect_delay * 1.5)

    async def notify_task_status(self, task_id, tenant_id, status, details=None, global_task_id=None):
        """Env√≠a notificaci√≥n de actualizaci√≥n de estado"""
        if not self.connected or not self.websocket:
            logger.warning("WebSocket no conectado. No se puede enviar notificaci√≥n.")
            return
            
        try:
            notification = {
                "event": "task_status_update",
                "service": self.service_name,
                "task_id": task_id,
                "global_task_id": global_task_id,
                "tenant_id": tenant_id,
                "timestamp": datetime.utcnow().isoformat(),
                "data": {
                    "status": status,
                    "details": details or {}
                }
            }
            
            await self.websocket.send(json.dumps(notification))
            logger.debug(f"Notificaci√≥n enviada: {notification['event']} para tarea {task_id}")
            
        except Exception as e:
            logger.error(f"Error al enviar notificaci√≥n de estado: {e}")
            self.connected = False
            
    async def notify_task_completion(self, task_id, tenant_id, result, global_task_id=None):
        """Notifica la finalizaci√≥n exitosa de una ingesti√≥n"""
        if not self.connected or not self.websocket:
            logger.warning("WebSocket no conectado. No se puede enviar notificaci√≥n.")
            return
            
        try:
            notification = {
                "event": "task_completed",
                "service": self.service_name,
                "task_id": task_id,
                "global_task_id": global_task_id,
                "tenant_id": tenant_id,
                "timestamp": datetime.utcnow().isoformat(),
                "data": result
            }
            
            await self.websocket.send(json.dumps(notification))
            logger.info(f"Tarea {task_id} completada y notificada")
            
        except Exception as e:
            logger.error(f"Error al notificar finalizaci√≥n de tarea: {e}")
            self.connected = False
```

#### Verificar Estado del Trabajo

```python
import requests

job_id = "job_123456"

response = requests.get(
    f"http://ingestion-service:8000/jobs/{job_id}",
    headers={"x-tenant-id": "tenant123"}
)

status = response.json()
print(f"Estado: {status['status']}")
print(f"Progreso: {status['progress']}%")
```

## üîå Sistema de Notificaciones

### WebSockets Centralizados

- **Integraci√≥n con orquestador**: Comunicaci√≥n bidireccional con Agent Orchestrator
- **Notificaciones de progreso**: Actualizaci√≥n en tiempo real del estado de procesamiento
- **Reconexi√≥n autom√°tica**: Mecanismo de backoff exponencial para mayor resiliencia
- **Autenticaci√≥n por token**: Comunicaci√≥n segura entre servicios

### Eventos Espec√≠ficos del Ingestion Service

- `document_ingestion_started`: Inicio del procesamiento de un documento
- `ingestion_progress_update`: Actualizaci√≥n de progreso con porcentaje
- `document_processing_completed`: Documento completamente procesado
- `document_ingestion_failed`: Error en el procesamiento del documento

### Implementaci√≥n WebSocket para Notificaciones:

```python
# websocket/notifier.py
import asyncio
import websockets
import json
import logging
from datetime import datetime

ORCHESTRATOR_WS_URL = "ws://agent-orchestrator:8000/ws/task_updates"

logger = logging.getLogger(__name__)

async def notify_ingestion_progress(job_id, tenant_id, progress_percentage, global_task_id=None):
    """Notifica el progreso de un trabajo de ingesti√≥n"""
    try:
        async with websockets.connect(ORCHESTRATOR_WS_URL) as websocket:
            notification = {
                "event": "ingestion_progress_update",
                "service": "ingestion",
                "task_id": job_id,
                "global_task_id": global_task_id,
                "tenant_id": tenant_id,
                "timestamp": datetime.utcnow().isoformat(),
                "data": {
                    "progress_percentage": progress_percentage,
                    "status": "processing" if progress_percentage < 100 else "completed"
                }
            }
            await websocket.send(json.dumps(notification))
    except Exception as e:
        logger.error(f"Error al notificar progreso via WebSocket: {e}")

async def notify_ingestion_completed(job_id, tenant_id, result, global_task_id=None):
    """Notifica la finalizaci√≥n de un trabajo de ingesti√≥n"""
    try:
        async with websockets.connect(ORCHESTRATOR_WS_URL) as websocket:
            notification = {
                "event": "document_processing_completed",
                "service": "ingestion",
                "task_id": job_id,
                "global_task_id": global_task_id,
                "tenant_id": tenant_id,
                "timestamp": datetime.utcnow().isoformat(),
                "data": result
            }
            await websocket.send(json.dumps(notification))
    except Exception as e:
        logger.error(f"Error al notificar completado via WebSocket: {e}")
```

## üåê Integraci√≥n en el Ecosistema

### Beneficios de la Arquitectura

- **Procesamiento especializado**: Optimizaci√≥n para diferentes tipos de documentos
- **Escalabilidad independiente**: El servicio puede escalarse seg√∫n la demanda de ingesti√≥n
- **Asincron√≠a completa**: Procesamiento en background sin bloquear otras operaciones
- **Flexibilidad en estrategias**: F√°cil adaptaci√≥n a diferentes necesidades de chunking y extracci√≥n

## Mejores Pr√°cticas

### Chunking Optimizado

El servicio implementa estrategias avanzadas de chunking:

1. **Chunking por fragmentos sem√°nticos**: Divide el texto respetando la estructura sem√°ntica
2. **Solapamiento configurable**: Mantiene contexto entre fragmentos
3. **Metadatos enriquecidos**: Incluye informaci√≥n sobre origen, posici√≥n y relaciones

### Procesamiento Eficiente

Para un procesamiento √≥ptimo:

1. **Tama√±o de documentos**: Los documentos no deben exceder el l√≠mite configurado (por defecto 20MB)
2. **Formato de documentos**: Preferir formatos estructurados como PDF, DOCX o HTML
3. **Colecciones**: Organizar documentos en colecciones tem√°ticas para mejor recuperaci√≥n
4. **Metadatos**: Proporcionar metadatos descriptivos (t√≠tulo, tags, descripci√≥n)

### Ingesta por Lotes

Para ingesta masiva:

1. **Endpoint de lotes**: Utilizar `/ingest/batch/urls` para procesar m√∫ltiples URLs
2. **Monitoreo de trabajos**: Verificar el estado mediante el endpoint `/jobs/{job_id}`
3. **Reintentos**: Implementar l√≥gica de reintentos en el cliente para documentos fallidos

## Flujo de Procesamiento

1. **Recepci√≥n**: El documento se recibe a trav√©s de un endpoint
2. **Validaci√≥n**: Se verifica formato, tama√±o y permisos
3. **Encolado**: Se crea un trabajo as√≠ncrono y se encola
4. **Extracci√≥n**: Un worker extrae el texto del documento
5. **Chunking**: El texto se divide en fragmentos √≥ptimos
6. **Embeddings**: Se solicitan embeddings al embedding-service
7. **Almacenamiento**: Los fragmentos con embeddings se almacenan en la base de datos
8. **Notificaci√≥n**: Se actualiza el estado del trabajo a completado

## Resoluci√≥n de Problemas

### Problemas Comunes

| Problema | Posible Causa | Soluci√≥n |
|----------|---------------|----------|
| Error 413 | Documento demasiado grande | Reducir tama√±o o dividir documento |
| Error 415 | Formato no soportado | Convertir a formato soportado (PDF, DOCX) |
| Error 429 | L√≠mite de rate excedido | Implementar backoff exponencial |
| Error 500 en extracci√≥n | Documento da√±ado o protegido | Verificar integridad y permisos |
| Timeout en procesamiento | Documento muy complejo | Aumentar timeout o dividir documento |

### Logs y Monitoreo

El servicio emite logs detallados sobre el proceso de ingesta:

- **INFO**: Eventos normales de procesamiento
- **WARNING**: Problemas no cr√≠ticos (reintentos, degradaci√≥n)
- **ERROR**: Fallos en procesamiento, conexiones o almacenamiento

## Integraci√≥n con Otros Servicios

- **Embedding Service**: Para generaci√≥n de embeddings
- **Query Service**: Utiliza los documentos procesados para responder consultas
- **Agent Service**: Orquesta el proceso completo de RAG

## Seguridad

- **Validaci√≥n de tenant**: Todos los endpoints requieren un tenant v√°lido
- **Sanitizaci√≥n de contenido**: Limpieza de contenido malicioso
- **L√≠mites por tier**: Restricciones seg√∫n el plan del tenant
- **Aislamiento de datos**: Estricta separaci√≥n entre tenants
