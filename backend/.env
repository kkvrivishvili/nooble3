# Configuración de Sistema Multi-Tenant
# ---------------------------
TENANT_ID=default                           # Identificador del tenant (usado para cargar configuraciones específicas)
LOAD_CONFIG_FROM_SUPABASE=false             # Si se deben cargar configuraciones desde Supabase
CONFIG_ENVIRONMENT=development              # Entorno: development, staging, production

# Configuración de Persistencia
# ----------------------------
REDIS_URL=redis://redis:6379/0              # URL para la conexión a Redis
SUPABASE_URL=https://naqhwsjrbyfojrlaholn.supabase.co # URL de Supabase
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5hcWh3c2pyYnlmb2pybGFob2xuIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY3MTIzNjQsImV4cCI6MjA2MjI4ODM2NH0.tvvxYwOdieEtHRbofuZ4n7gKrE8ITQWq1JmWLUO5csk        # Clave de API para Supabase
SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5hcWh3c2pyYnlmb2pybGFob2xuIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0NjcxMjM2NCwiZXhwIjoyMDYyMjg4MzY0fQ.UYxF1q0VutZP290vzu11nVPsaVMt2wTIb3S4OIAzGn8


# Configuración de Modelos y API
# ---------------------------
# Selección de proveedor de AI (Ollama, OpenAI, Groq)
USE_OLLAMA=true                             # true: usa Ollama, false: usa OpenAI o Groq
USE_GROQ=false                              # true: usa Groq, false: usa OpenAI o Ollama

# Configuración de Ollama
OLLAMA_HOST=ollama                          # Hostname del servicio Ollama
OLLAMA_PORT=11434                           # Puerto del servicio Ollama
OLLAMA_API_URL=http://ollama:11434          # URL de la API de Ollama
OLLAMA_WAIT_TIMEOUT=300                     # Tiempo máximo (seg) de espera para que Ollama esté listo
OLLAMA_PULL_MODELS=true                     # Si se deben descargar modelos al inicio
DEFAULT_OLLAMA_MODEL=llama3.2:1b              # Modelo LLM predeterminado para Ollama
DEFAULT_OLLAMA_LLM_MODEL=llama3.2:1b          # Modelo LLM predeterminado para Ollama
DEFAULT_OLLAMA_EMBEDDING_MODEL=nomic-embed-text # Modelo de embedding para Ollama

# Configuración de OpenAI
OPENAI_API_KEY=sk-your-openai-key-here      # Clave de API para OpenAI
DEFAULT_OPENAI_MODEL=gpt-3.5-turbo          # Modelo predeterminado para OpenAI
DEFAULT_OPENAI_LLM_MODEL=gpt-3.5-turbo      # Modelo LLM predeterminado para OpenAI
DEFAULT_OPENAI_EMBEDDING_MODEL=text-embedding-3-small # Modelo de embedding para OpenAI

# Configuración de Groq
GROQ_API_KEY=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  # Clave de API para Groq
DEFAULT_GROQ_MODEL=llama3-70b-8192          # Modelo predeterminado para Groq
DEFAULT_GROQ_LLM_MODEL=llama3-70b-8192      # Modelo LLM predeterminado para Groq

# Configuración de Servicios
# -------------------------
# URLs de servicios para comunicación entre microservicios
EMBEDDING_SERVICE_URL=http://embedding-service:8001
QUERY_SERVICE_URL=http://query-service:8002
AGENT_SERVICE_URL=http://agent-service:8003
INGESTION_SERVICE_URL=http://ingestion-service:8000

# Puertos para los servicios (solo se usan si SERVICE_NAME no está definido)
INGESTION_SERVICE_PORT=8000
EMBEDDING_SERVICE_PORT=8001
QUERY_SERVICE_PORT=8002
AGENT_SERVICE_PORT=8003

# Identificación de servicio (usado para determinar automáticamente la configuración del servicio)
SERVICE_NAME=                               # Dejar vacío para autodetección o establecer a: ingestion-service, embedding-service, query-service, agent-service

# Configuración de Entorno
# -----------------------
TESTING_MODE=false                          # Modo de testing (true para desarrollo, false para producción)
MOCK_OPENAI=false                           # Simular respuestas de OpenAI (solo para pruebas)
# SKIP_SUPABASE=false                       # [OBSOLETO] Esta variable ya no se utiliza
LOG_LEVEL=INFO                              # Nivel de logging (DEBUG, INFO, WARNING, ERROR, CRITICAL)

# Límites y Cuotas
# ---------------
DEFAULT_RATE_LIMIT=100                      # Límite de peticiones por minuto por usuario
MAX_TOKENS_PER_PROMPT=2048                  # Máximo número de tokens por prompt
RATE_LIMIT_FREE_TIER=600                    # Peticiones por minuto para tier gratuito
RATE_LIMIT_PRO_TIER=1200                    # Peticiones por minuto para tier pro
RATE_LIMIT_BUSINESS_TIER=3000               # Peticiones por minuto para tier business

# Configuración de Caché
# ---------------------
CACHE_TTL=86400                             # Tiempo de vida por defecto en segundos (24 horas)
EMBEDDING_CACHE_TTL=604800                  # TTL para embeddings en segundos (7 días)
QUERY_CACHE_TTL=3600                        # TTL para resultados de consultas en segundos (1 hora)
SETTINGS_TTL=300                            # TTL para caché de configuraciones (5 minutos)
CACHE_TTL_SHORT=300                         # TTL para caché corta (5 minutos)
CACHE_TTL_STANDARD=3600                     # TTL para caché estándar (1 hora)
CACHE_TTL_EXTENDED=86400                    # TTL para caché extendida (24 horas)
USE_MEMORY_CACHE=true                       # Si se debe usar caché en memoria
MEMORY_CACHE_SIZE=1000                      # Tamaño máximo de caché en memoria (entradas)
MEMORY_CACHE_CLEANUP_PERCENT=0.2            # Porcentaje a limpiar cuando se excede el tamaño
REDIS_MAX_CONNECTIONS=10                    # Máximo número de conexiones a Redis

# Configuración de colas y trabajos
# --------------------------------
JOB_LOCK_EXPIRE_SECONDS=600                 # Tiempo máximo para un trabajo (10 minutos)
MAX_WORKERS=3                               # Número máximo de workers para procesamiento

# Factores de costo para modelos
# -----------------------------
# JSON con factores de costo para diferentes modelos. Ejemplo válido:
MODEL_COST_FACTORS={"gpt-3.5-turbo":1.0,"gpt-4-turbo":5.0,"claude-3-5-sonnet":8.0,"llama3":0.8,"llama3:70b":2.0}

# Configuración HTTP y conexiones
# ------------------------------
HTTP_TIMEOUT=30                             # Timeout para solicitudes HTTP en segundos
MAX_RETRIES=3                               # Número máximo de reintentos para solicitudes fallidas
RETRY_BACKOFF=1.5                           # Factor de incremento entre reintentos

# Tracking y métricas
# ------------------
ENABLE_USAGE_TRACKING=true                  # Habilitar seguimiento de uso (tokens, consultas)
ENABLE_PERFORMANCE_TRACKING=true            # Habilitar seguimiento de rendimiento

# Configuración específica de servicios
# ------------------------------------
# Agent Service
AGENT_DEFAULT_MESSAGE_LIMIT=50              # Límite predeterminado de mensajes por conversación
AGENT_STREAMING_TIMEOUT=300                 # Tiempo máximo (seg) para streaming de respuestas

# Embedding Service
EMBEDDING_CACHE_ENABLED=true                # Habilitar caché para embeddings
EMBEDDING_BATCH_SIZE=100                    # Tamaño de lote para procesar embeddings
MAX_EMBEDDING_BATCH_SIZE=10                 # Tamaño máximo de lote para API de embeddings  

# Query Service
DEFAULT_SIMILARITY_TOP_K=4                  # Número predeterminado de resultados similares
SIMILARITY_THRESHOLD=0.7                    # Umbral de similitud para considerar resultados relevantes
MAX_QUERY_RETRIES=3                         # Máximo número de reintentos para consultas
CHUNK_SIZE=512                              # Tamaño de fragmentos para indexación
CHUNK_OVERLAP=51                            # Solapamiento entre fragmentos consecutivos

# Ingestion Service
MAX_DOC_SIZE_MB=10                          # Tamaño máximo de documentos a procesar