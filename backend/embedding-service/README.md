# Embedding Service

## DescripciÃ³n
Microservicio optimizado que proporciona capacidades de generaciÃ³n de embeddings vectoriales para el sistema RAG (Retrieval Augmented Generation), utilizando exclusivamente modelos de OpenAI. Este servicio es fundamental para transformar texto en representaciones numÃ©ricas que permiten la bÃºsqueda semÃ¡ntica en el sistema.

## ğŸ—ï¸ Ecosistema de Servicios

La arquitectura se organiza en 3 niveles jerÃ¡rquicos:

### Nivel 1: OrquestaciÃ³n

- **Agent Orchestrator**: Punto de entrada Ãºnico, gestiÃ³n de sesiones y coordinaciÃ³n global

### Nivel 2: Servicios Funcionales

- **Conversation Service**: Historial y contexto de conversaciones
- **Workflow Engine**: Flujos de trabajo complejos multi-etapa
- **Agent Execution**: LÃ³gica especÃ­fica del agente
- **Tool Registry**: Registro y ejecuciÃ³n de herramientas

### Nivel 3: Servicios de Infraestructura

- **Query Service**: Procesamiento RAG y LLM
- **Embedding Service**: GeneraciÃ³n de embeddings vectoriales
- **Ingestion Service**: Procesamiento de documentos

> ğŸ“Œ **Este documento describe el Embedding Service**, ubicado en el Nivel 3 como servicio de infraestructura especializado en la generaciÃ³n de vectores semÃ¡nticos

## CaracterÃ­sticas

- GeneraciÃ³n de embeddings de alta calidad utilizando modelos de OpenAI
- Ãšnico endpoint optimizado para facilitar la integraciÃ³n
- Soporte para tracking centralizado de tokens
- Validaciones de seguridad y lÃ­mites de uso
- Manejo de errores estandarizado

## Arquitectura

El servicio sigue una arquitectura simplificada:

```
Agent Service
    â†“
EnhancedEmbeddingRequest
    â†“
Embedding Service (validate)
    â†“
OpenAI Provider (generate)
    â†“
EnhancedEmbeddingResponse
    â†“
Agent Service â†’ Query Service
```

## ğŸ”„ Flujos de Trabajo Principales

### 1. Consulta Normal (ParticipaciÃ³n del Embedding Service)
```
Cliente â†’ Orchestrator â†’ Agent Execution â†’ Embedding Service â†’ Query â†’ Respuesta
```

### 2. IngestiÃ³n de Documentos
```
Cliente â†’ Orchestrator â†’ Workflow Engine â†’ Ingestion Service â†’ Embedding Service â†’ NotificaciÃ³n de completado
```

> ğŸ” **Rol del Embedding Service**: Transformar texto en representaciones vectoriales que permiten bÃºsquedas semÃ¡nticas y contextualmente relevantes en el sistema RAG.

## Estructura

```
embedding-service/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ embeddings.py          # EmbeddingRequest, EmbeddingResponse
â”œâ”€â”€ provider/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ openai.py              # ImplementaciÃ³n del proveedor de OpenAI
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embeddings.py          # Endpoint Ãºnico de API
â”‚   â””â”€â”€ health.py              # Health check
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py            # Configuraciones centralizadas
â”œâ”€â”€ queue/                   # Sistema de cola de trabajo (para implementar)
â”‚   â”œâ”€â”€ __init__.py             # InicializaciÃ³n del mÃ³dulo
â”‚   â”œâ”€â”€ worker.py               # ConfiguraciÃ³n de workers
â”‚   â”œâ”€â”€ producer.py             # MÃ©todos para encolar tareas
â”‚   â””â”€â”€ tasks.py                # DefiniciÃ³n de tareas asÃ­ncronas
â”œâ”€â”€ main.py                 # Punto de entrada de la aplicaciÃ³n FastAPI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

## InstalaciÃ³n

```bash
# Clonar el repositorio
git clone <repositorio>

# Instalar dependencias
cd embedding-service
pip install -r requirements.txt

# Ejecutar el servicio
uvicorn main:app --host 0.0.0.0 --port 8001
```

## ConfiguraciÃ³n

El servicio utiliza variables de entorno para la configuraciÃ³n:

- `EMBEDDING_OPENAI_API_KEY`: Clave de API para OpenAI (requerida)
- `EMBEDDING_DEFAULT_EMBEDDING_MODEL`: Modelo predeterminado (default: "text-embedding-3-small")
- `EMBEDDING_MAX_BATCH_SIZE`: TamaÃ±o mÃ¡ximo de lote (default: 100)
- `EMBEDDING_MAX_TEXT_LENGTH`: Longitud mÃ¡xima de texto (default: 8000)
- `EMBEDDING_OPENAI_TIMEOUT_SECONDS`: Timeout para llamadas a OpenAI (default: 30)

## Funciones Clave
1. GeneraciÃ³n de embeddings de alta calidad utilizando modelos de OpenAI
2. Procesamiento por lotes de textos para optimizar rendimiento
3. Tracking de uso de tokens por tenant y operaciÃ³n
4. ValidaciÃ³n y normalizaciÃ³n de textos para embeddings

## Sistema de Cola de Trabajo

Para manejar de manera eficiente las solicitudes de embeddings por lotes, especialmente para operaciones de ingestado de documentos, el Embedding Service implementa un sistema simple de cola de trabajo basado en Redis Queue (RQ).

## ğŸš¦ Sistema de Colas Multi-tenant

### Estructura JerÃ¡rquica de Colas del Embedding Service

```
+-------------------------------------+
|          COLAS DE EMBEDDING         |
+-------------------------------------+
|                                     |
| embedding_tasks:{tenant_id}         | â†’ Cola principal de tareas
| embedding_results:{tenant_id}:{id}  | â†’ Resultados temporales
| embedding_batch:{tenant_id}:{batch} | â†’ Procesos de ingestado
|                                     |
+-------------------------------------+
```

### CaracterÃ­sticas Clave

- **SegmentaciÃ³n por tenant**: Completo aislamiento de datos entre tenants
- **Procesamiento por lotes**: OptimizaciÃ³n para grandes volÃºmenes de texto
- **PriorizaciÃ³n de tareas**: Consultas interactivas priorizadas sobre ingestado masivo
- **Tracking de tokens**: Monitoreo detallado del uso por tenant

### ImplementaciÃ³n del Sistema de Colas:

```python
# queue/job_manager.py
import uuid
from redis import Redis
from rq import Queue

# ConfiguraciÃ³n de Redis y cola Ãºnica
redis_conn = Redis(host=os.getenv('REDIS_HOST', 'redis'), 
                  port=int(os.getenv('REDIS_PORT', 6379)), 
                  db=int(os.getenv('REDIS_DB', 0)))
embedding_queue = Queue('embedding_tasks', connection=redis_conn)

# Registro simple de trabajos
job_registry = {}

def enqueue_embedding_task(texts, tenant_id, collection_id=None, metadata=None):
    """Encola una tarea de generaciÃ³n de embeddings y devuelve un job_id"""
    from provider.openai import generate_embeddings
    
    job_id = str(uuid.uuid4())
    job_data = {
        "texts": texts,
        "tenant_id": tenant_id,
        "collection_id": collection_id,
        "metadata": metadata or {}
    }
    
    # Registrar trabajo
    job_registry[job_id] = {"status": "pending"}
    
    # Encolar tarea
    job = embedding_queue.enqueue(
        generate_embeddings,
        job_data,
        job_id=job_id,
        result_ttl=3600  # Mantener resultado 1 hora
    )
    
    # Callback cuando finalice (empleando meta)
    job.meta['job_id'] = job_id
    job.meta['tenant_id'] = tenant_id
    job.save_meta()
    
    return {"job_id": job_id}
```

### Worker Simple:

```python
# queue/worker.py
import os
from redis import Redis
from rq import Worker, Queue, Connection
from websocket.notifier import notify_job_completed

def process_result(job, connection, result, exception=None):
    """Maneja el resultado del trabajo y notifica al orquestador"""
    job_id = job.meta.get('job_id')
    tenant_id = job.meta.get('tenant_id')
    
    if exception is None and job_id:
        # Notificar Ã©xito vÃ­a WebSocket
        notify_job_completed(job_id, result, tenant_id)

# ConfiguraciÃ³n del worker
redis_conn = Redis(
    host=os.getenv('REDIS_HOST', 'redis'),
    port=int(os.getenv('REDIS_PORT', 6379)),
    db=int(os.getenv('REDIS_DB', 0))
)

def run_worker():
    with Connection(redis_conn):
        worker = Worker(['embedding_tasks'], exception_handlers=[process_result])
        worker.work(with_scheduler=True)

if __name__ == '__main__':
    run_worker()
```

## ğŸ”Œ Sistema de Notificaciones

### WebSockets Centralizados

- **Hub de conexiÃ³n**: IntegraciÃ³n con el servidor WebSocket centralizado del Agent Orchestrator
- **NotificaciÃ³n automÃ¡tica**: ActualizaciÃ³n en tiempo real cuando los embeddings estÃ¡n listos
- **ReconexiÃ³n inteligente**: Mecanismo de backoff exponencial para conexiones robustas
- **AutenticaciÃ³n por token**: Seguridad en las comunicaciones inter-servicio

### Eventos EspecÃ­ficos del Embedding Service

- `embeddings_generated`: Vectores generados exitosamente
- `batch_progress_update`: ActualizaciÃ³n de progreso en procesos por lotes
- `embedding_failed`: Error en la generaciÃ³n de embeddings

### IntegraciÃ³n con WebSocket para Notificaciones:

```python
# websocket/notifier.py
import asyncio
import websockets
import json

ORCHESTRATOR_WS_URL = "ws://agent-orchestrator:8000/ws/task_updates"

async def notify_job_completed(job_id, result, tenant_id):
    """Notifica al orquestador que un trabajo ha terminado"""
    try:
        async with websockets.connect(ORCHESTRATOR_WS_URL) as websocket:
            notification = {
                "event": "job_completed",
                "service": "embedding",
                "job_id": job_id,
                "tenant_id": tenant_id,
                "result": result
            }
            await websocket.send(json.dumps(notification))
    except Exception as e:
        print(f"Error al notificar via WebSocket: {e}")
```

### AdiciÃ³n al requirements.txt:
```
rq==1.15.1
redis>=4.5.1
websockets>=10.0
```

## ComunicaciÃ³n
- **HTTP**: API REST para iniciar tareas (endpoints sÃ­ncronos y asÃ­ncronos)
- **WebSocket**: Notificaciones en tiempo real al Agent Orchestrator cuando se completan tareas asÃ­ncronas
- **ComunicaciÃ³n Centralizada**: Toda comunicaciÃ³n pasa a travÃ©s del Agent Orchestrator Service

## IntegraciÃ³n con otros Servicios
El Embedding Service se comunica **principalmente** con:

1. **Agent Orchestrator Service**: Como punto de entrada principal para solicitudes de embeddings.
2. **Ingestion Service**: Ãšnica excepciÃ³n que puede comunicarse directamente para procesar documentos durante la ingestiÃ³n.

Aspectos clave de la integraciÃ³n:
- No se comunica directamente con Query Service (esta comunicaciÃ³n es gestionada por el orquestador)
- Mantiene un tracking centralizado de uso de tokens OpenAI
- Opera exclusivamente con modelos de embeddings de OpenAI para mantener consistencia

## API

### Endpoint: `/api/v1/internal/enhanced_embed`

**MÃ©todo**: POST

**DescripciÃ³n**: Genera embeddings para los textos proporcionados.

**Request Body**:

```json
{
  "texts": ["Texto para generar embedding"],
  "model": "text-embedding-3-large",  // Opcional
  "tenant_id": "tenant123",
  "collection_id": "collection456",  // Opcional, para tracking
  "chunk_ids": ["chunk1", "chunk2"],  // Opcional, para tracking
  "metadata": {                       // Opcional
    "source": "agent_service",
    "purpose": "query"
  }
}
```

**Response**:

```json
{
  "success": true,
  "message": "Embeddings generados correctamente",
  "embeddings": [[0.1, 0.2, ...]],
  "model": "text-embedding-3-large",
  "dimensions": 3072,
  "processing_time": 0.156,
  "total_tokens": 10
}
```

## IntegraciÃ³n con Agent Service

El Agent Service utiliza este servicio como herramienta para generar embeddings de consultas y respuestas en el flujo RAG. La integraciÃ³n tÃ­pica es:

### Ejemplo de CÃ³digo de IntegraciÃ³n:

```python
import httpx
from models import EnhancedEmbeddingRequest

async def get_embeddings_for_query(query_text, tenant_id, collection_id=None):
    """Obtiene embeddings para una consulta de usuario."""
    
    request = EnhancedEmbeddingRequest(
        texts=[query_text],
        tenant_id=tenant_id,
        collection_id=collection_id,
        metadata={"source": "agent_service", "purpose": "query"}
    )
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://embedding-service:8001/api/v1/internal/enhanced_embed",
            json=request.dict(exclude_none=True)
        )
        
    if response.status_code == 200:
        result = response.json()
        return result["embeddings"][0]  # Primer embedding
    else:
        raise ValueError(f"Error en embedding service: {response.text}")
```

## ğŸŒ IntegraciÃ³n en el Ecosistema

### Formato Estandarizado de Mensajes

```json
{
  "task_id": "uuid-v4",
  "tenant_id": "tenant-identifier",
  "created_at": "ISO-timestamp",
  "status": "pending|processing|completed|failed",
  "service": "embedding",
  "metadata": {
    "collection_id": "optional-collection-id",
    "model": "text-embedding-3-small|text-embedding-3-large",
    "source": "query|ingestion|batch",
    "priority": 0-9
  },
  "payload": {
    "texts": ["texto1", "texto2"],
    "dimensions": 1536,
    "batch_size": 100
  }
}
```

### Flujo de Trabajo en el Sistema RAG

1. **Agent Service** recibe consulta del usuario
2. **Agent Service** solicita embeddings al **Embedding Service**
3. **Embedding Service** genera vectores utilizando OpenAI
4. **Agent Service** utiliza embeddings para buscar en el **Query Service**
5. **Query Service** encuentra documentos relevantes
6. **Agent Service** genera respuesta con contexto enriquecido

### Beneficios de la Arquitectura

- **Escalabilidad**: Servicio especializado que puede escalarse independientemente
- **Resilencia**: Fallos aislados no afectan a todo el sistema
- **OptimizaciÃ³n**: Procesamiento por lotes para eficiencia en costos y rendimiento
- **Flexibilidad**: FÃ¡cil cambio de proveedor de embeddings sin afectar otros servicios

## Monitoreo y MÃ©tricas

El servicio registra automÃ¡ticamente:
- Uso de tokens por tenant y modelo
- Tiempos de respuesta
- Errores y excepciones

## Soporte de Modelos

El servicio soporta los siguientes modelos de OpenAI:

| Modelo | Dimensiones | Recomendado para |
|--------|------------|------------------|
| text-embedding-3-small | 1536 | Uso general, mejor balance costo/rendimiento |
| text-embedding-3-large | 3072 | Alta precisiÃ³n, tareas complejas |
| text-embedding-ada-002 | 1536 | Compatibilidad con sistemas legacy |

## Buenas PrÃ¡cticas

1. **Batch de solicitudes**: Cuando sea posible, enviar mÃºltiples textos en una sola solicitud.
2. **TamaÃ±o de texto**: Mantener textos dentro de lÃ­mites razonables (<8000 caracteres).
3. **Tenant ID**: Siempre especificar un tenant_id vÃ¡lido para tracking correcto.
4. **Manejo de errores**: Implementar reintentos con backoff exponencial para errores transitorios.
