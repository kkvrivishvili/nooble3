version: '3.8'

services:

  redis:
    build:
      context: ./docker/redis
      dockerfile: Dockerfile
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: always
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # El servicio Ollama ha sido eliminado ya que no es compatible con la arquitectura actual
  # que usa exclusivamente OpenAI y Groq para los modelos de embeddings y LLM

  embedding-service:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.embedding
    restart: always
    ports:
      - "8001:8001"
    volumes:
      - ./common:/app/common
      - ./embedding-service:/app/embedding-service
      - ./logs:/app/logs
    dns:
      - 8.8.8.8
      - 1.1.1.1
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - SERVICE_NAME=embedding-service
      - DOCKER_CONTAINER=true
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=INFO
      - USE_GROQ=${USE_GROQ:-false}
      # Variables de Ollama eliminadas - usando exclusivamente OpenAI y Groq
      - DEFAULT_GROQ_MODEL=${DEFAULT_GROQ_MODEL:-llama3-70b-8192}
      - SUPABASE_URL=postgresql://postgres:postgres@supabase-dev:54321/postgres
    depends_on:
      redis:
        condition: service_healthy
      # La dependencia de Ollama ha sido eliminada

  query-service:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.query
    restart: always
    ports:
      - "8002:8002"
    volumes:
      - ./common:/app/common
      - ./query-service:/app/query-service
      - ./logs:/app/logs
    dns:
      - 8.8.8.8
      - 1.1.1.1
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - SERVICE_NAME=query-service
      - DOCKER_CONTAINER=true
      - REDIS_URL=redis://redis:6379/0
      - EMBEDDING_SERVICE_URL=http://embedding-service:8001
      - LOG_LEVEL=INFO
      - USE_GROQ=${USE_GROQ:-false}
      # Variables de Ollama eliminadas - usando exclusivamente OpenAI y Groq
      - DEFAULT_GROQ_MODEL=${DEFAULT_GROQ_MODEL:-llama3-70b-8192}
      - DEFAULT_GROQ_LLM_MODEL=${DEFAULT_GROQ_LLM_MODEL:-llama3-70b-8192}
    depends_on:
      redis:
        condition: service_healthy
      embedding-service:
        condition: service_started
      # La dependencia de Ollama ha sido eliminada

  agent-service:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.agent
    restart: always
    ports:
      - "8003:8003"
    volumes:
      - ./common:/app/common
      - ./agent-service:/app/agent-service
      - ./logs:/app/logs
    dns:
      - 8.8.8.8
      - 1.1.1.1
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - SERVICE_NAME=agent-service
      - DOCKER_CONTAINER=true
      - REDIS_URL=redis://redis:6379/0
      - EMBEDDING_SERVICE_URL=http://embedding-service:8001
      - QUERY_SERVICE_URL=http://query-service:8002
      - LOG_LEVEL=INFO
      - USE_GROQ=${USE_GROQ:-false}
      # Variables de Ollama eliminadas - usando exclusivamente OpenAI y Groq
      - DEFAULT_GROQ_MODEL=${DEFAULT_GROQ_MODEL:-llama3-70b-8192}
      - DEFAULT_GROQ_LLM_MODEL=${DEFAULT_GROQ_LLM_MODEL:-llama3-70b-8192}
      - JOB_LOCK_EXPIRE_SECONDS=600
    depends_on:
      redis:
        condition: service_healthy
      embedding-service:
        condition: service_started
      query-service:
        condition: service_started
      # La dependencia de Ollama ha sido eliminada

  ingestion-service:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.ingestion
    restart: always
    ports:
      - "8000:8000"
    volumes:
      - ./common:/app/common
      - ./ingestion-service:/app/ingestion-service
      - ./logs:/app/logs
    dns:
      - 8.8.8.8
      - 1.1.1.1
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - SERVICE_NAME=ingestion-service
      - DOCKER_CONTAINER=true
      - REDIS_URL=redis://redis:6379/0
      - EMBEDDING_SERVICE_URL=http://embedding-service:8001
      - QUERY_SERVICE_URL=http://query-service:8002
      - LOG_LEVEL=INFO
      - USE_GROQ=${USE_GROQ:-false}
      # Variables de Ollama eliminadas - usando exclusivamente OpenAI y Groq
      - DEFAULT_GROQ_MODEL=${DEFAULT_GROQ_MODEL:-llama3-70b-8192}
      - DEFAULT_GROQ_LLM_MODEL=${DEFAULT_GROQ_LLM_MODEL:-llama3-70b-8192}
      - JOB_LOCK_EXPIRE_SECONDS=600
    depends_on:
      redis:
        condition: service_healthy
      embedding-service:
        condition: service_started
      # La dependencia de Ollama ha sido eliminada

  worker-service:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.worker
    restart: always
    ports:
      - "8080:8080"
    volumes:
      - ./common:/app/common
      - ./worker-service:/app/worker-service
      - ./logs:/app/logs
    dns:
      - 8.8.8.8
      - 1.1.1.1
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - SERVICE_NAME=worker-service
      - DOCKER_CONTAINER=true
      - REDIS_URL=redis://redis:6379/0
      - EMBEDDING_SERVICE_URL=http://embedding-service:8001
      - QUERY_SERVICE_URL=http://query-service:8002
      - AGENT_SERVICE_URL=http://agent-service:8003
      - INGESTION_SERVICE_URL=http://ingestion-service:8000
      # OLLAMA_API_URL ya no es soportado - el sistema usa exclusivamente OpenAI y Groq
      - LOG_LEVEL=INFO
      - MAX_WORKERS=3
    depends_on:
      redis:
        condition: service_healthy
      embedding-service:
        condition: service_started
      query-service:
        condition: service_started
      agent-service:
        condition: service_started
      ingestion-service:
        condition: service_started

volumes:
  supabase-data:
  redis-data:
  # El volumen ollama-data ya no es necesario
  logs: