# Configuración de Sistema Multi-Tenant
# ---------------------------
TENANT_ID=default                           # Identificador del tenant (usado para cargar configuraciones específicas)
LOAD_CONFIG_FROM_SUPABASE=false             # Si se deben cargar configuraciones desde Supabase
CONFIG_ENVIRONMENT=development              # Entorno: development, staging, production

# Configuración de Persistencia
# ----------------------------
REDIS_URL=redis://redis:6379/0              # URL para la conexión a Redis
SUPABASE_URL=https://naqhwsjrbyfojrlaholn.supabase.co # URL de Supabase
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJsdGtnYmp1ZGd5aGVndm1xbG5nIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY3NDkzODQsImV4cCI6MjA2MjMyNTM4NH0.3_0rYjeracQzNU1gj_g_QAMKgUAobHEzH5vwZ-hG1cM        # Clave de API para Supabase
SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJsdGtnYmp1ZGd5aGVndm1xbG5nIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0Njc0OTM4NCwiZXhwIjoyMDYyMzI1Mzg0fQ.j-1GrGG7AcuKVn51pLNNlR7zXQD3iWVO0Irj_-9ONP0


# Configuración de Modelos y API
# ---------------------------
# Selección de proveedor de AI
# USE_OLLAMA ya no es soportado - El sistema utiliza exclusivamente OpenAI y Groq
USE_GROQ=false                              # true: usa Groq, false: no usa Groq

# Configuración de Ollama - OBSOLETA
# Estas variables ya no son soportadas. El sistema usa exclusivamente OpenAI y Groq.
# OLLAMA_HOST, OLLAMA_PORT, OLLAMA_API_URL, OLLAMA_WAIT_TIMEOUT, OLLAMA_PULL_MODELS,
# DEFAULT_OLLAMA_MODEL, DEFAULT_OLLAMA_LLM_MODEL y DEFAULT_OLLAMA_EMBEDDING_MODEL
# han sido eliminadas

# Configuración de OpenAI
OPENAI_API_KEY=sk-your-openai-key-here      # Clave de API para OpenAI
DEFAULT_OPENAI_MODEL=gpt-3.5-turbo          # Modelo predeterminado para OpenAI
DEFAULT_OPENAI_LLM_MODEL=gpt-3.5-turbo      # Modelo LLM predeterminado para OpenAI
DEFAULT_OPENAI_EMBEDDING_MODEL=text-embedding-3-small # Modelo de embedding para OpenAI

# Configuración de Groq
GROQ_API_KEY=gsk-your-groq-key-here         # Clave de API para Groq
DEFAULT_GROQ_MODEL=llama3-70b-8192          # Modelo predeterminado para Groq
DEFAULT_GROQ_LLM_MODEL=llama3-70b-8192      # Modelo LLM predeterminado para Groq

# Configuración de Servicios
# -------------------------
# URLs de servicios para comunicación entre microservicios
EMBEDDING_SERVICE_URL=http://embedding-service:8001
QUERY_SERVICE_URL=http://query-service:8002
AGENT_SERVICE_URL=http://agent-service:8003
INGESTION_SERVICE_URL=http://ingestion-service:8000

# Puertos para los servicios (solo se usan si SERVICE_NAME no está definido)
INGESTION_SERVICE_PORT=8000
EMBEDDING_SERVICE_PORT=8001
QUERY_SERVICE_PORT=8002
AGENT_SERVICE_PORT=8003

# Identificación de servicio (usado para determinar automáticamente la configuración del servicio)
SERVICE_NAME=                               # Dejar vacío para autodetección o establecer a: ingestion-service, embedding-service, query-service, agent-service

# Configuración de Entorno
# -----------------------
TESTING_MODE=false                          # Modo de testing (true para desarrollo, false para producción)
MOCK_OPENAI=false                           # Simular respuestas de OpenAI (solo para pruebas)
# SKIP_SUPABASE=false                       # [OBSOLETO] Esta variable ya no se utiliza
LOG_LEVEL=INFO                              # Nivel de logging (DEBUG, INFO, WARNING, ERROR, CRITICAL)

# Límites y Cuotas
# ---------------
DEFAULT_RATE_LIMIT=100                      # Límite de peticiones por minuto por usuario
MAX_TOKENS_PER_PROMPT=2048                  # Máximo número de tokens por prompt
RATE_LIMIT_FREE_TIER=600                    # Peticiones por minuto para tier gratuito
RATE_LIMIT_PRO_TIER=1200                    # Peticiones por minuto para tier pro
RATE_LIMIT_BUSINESS_TIER=3000               # Peticiones por minuto para tier business

# Configuración de Caché
# ---------------------
CACHE_TTL=86400                             # Tiempo de vida por defecto en segundos (24 horas)
EMBEDDING_CACHE_TTL=604800                  # TTL para embeddings en segundos (7 días)
QUERY_CACHE_TTL=3600                        # TTL para resultados de consultas en segundos (1 hora)

# Configuración de colas y trabajos
# --------------------------------
JOB_LOCK_EXPIRE_SECONDS=600                 # Tiempo máximo para un trabajo (10 minutos)
MAX_WORKERS=3                               # Número máximo de workers para procesamiento

# Factores de costo para modelos
# -----------------------------
# JSON con factores de costo para diferentes modelos. Ejemplo válido:
MODEL_COST_FACTORS={"gpt-3.5-turbo":1.0,"gpt-4-turbo":5.0,"claude-3-5-sonnet":8.0,"llama3":0.8,"llama3:70b":2.0}

# Configuración HTTP y conexiones
# ------------------------------
HTTP_TIMEOUT=30                             # Timeout para solicitudes HTTP en segundos
MAX_RETRIES=3                               # Número máximo de reintentos para solicitudes fallidas
RETRY_BACKOFF=1.5                           # Factor de incremento entre reintentos

# Tracking y métricas
# ------------------
ENABLE_USAGE_TRACKING=true                  # Habilitar seguimiento de uso (tokens, consultas)
ENABLE_PERFORMANCE_TRACKING=true            # Habilitar seguimiento de rendimiento